{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ed10706f831c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "# from Bio import SeqIO \n",
    "\n",
    "# import sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "We are going to use similiar thinking to sentence construction.\n",
    "https://chriskhanhtran.github.io/posts/cnn-sentence-classification/\n",
    "\n",
    "Data from: https://europepmc.org/article/med/16122420"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(seqs):\n",
    "    '''\n",
    "    Given a list of sequences, will turn into a tokenized vector.\n",
    "    \n",
    "    ARGS:\n",
    "        seqs: a list of strings where every string is a sequence\n",
    "    RETURNS:\n",
    "        tokenized_seqs (list(list(str))): list of list of tokens\n",
    "        voc2ind (dict) a dictionary where keys are letters, values are the corresponding token\n",
    "    '''\n",
    "    max_len = 0\n",
    "    \n",
    "    # build up a voc2ind (letters:token)\n",
    "    # based on ATGC and include padding and unknown tokens\n",
    "    voc2ind = {voc:ind for ind,voc in enumerate(['<pad>', '<unk>', 'A', 'T', 'C', 'G'])}\n",
    "    \n",
    "    i = len(voc2ind)\n",
    "    \n",
    "    # tokenize the sequences\n",
    "    tokenized_seqs = []\n",
    "    for seq in seqs:\n",
    "        tokenized_seq = []\n",
    "        for e in seq:\n",
    "            # make sure the sequence is upper case, a == A\n",
    "            seq = seq.upper()\n",
    "            # if we haven't seen this letter before, add to the corupus\n",
    "            if not e in voc2ind:\n",
    "                voc2ind[e] = i\n",
    "                i += 1\n",
    "            tokenized_seq.append(voc2ind[e])\n",
    "        tokenized_seqs.append(tokenized_seq)\n",
    "        \n",
    "    return tokenized_seqs, voc2ind\n",
    "        \n",
    "res = prepare_data(['ATCG', 'TAGA', 'APO'])\n",
    "print(res)\n",
    "assert(res[0] == [[2, 3, 4, 5], [3, 2, 5, 2], [2, 6, 7]]), res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(tokenized_seqs, voc2ind):\n",
    "    '''\n",
    "    Pad each sequence to the maximum length by adding a <pad> token\n",
    "    \n",
    "    ARGS:\n",
    "        tokenized_seqs (list(list(str))): list of list of tokens\n",
    "        voc2ind (dict) a dictionary where keys are letters, values are the corresponding token\n",
    "    RETURNS:\n",
    "        a numpy array of all the tokenized sequences that have been padded to be the same\n",
    "        length.\n",
    "    '''\n",
    "\n",
    "    padded_seqs = []\n",
    "    \n",
    "    # find max sequence length\n",
    "    max_len = 0\n",
    "    for seq in tokenized_seqs:\n",
    "        max_len = max(len(seq), max_len)\n",
    "    \n",
    "    # add padding so sequences are max_length\n",
    "    for seq in tokenized_seqs:\n",
    "        padded_seq = seq + [voc2ind['<pad>']] * (max_len - len(seq))\n",
    "        padded_seqs.append(padded_seq)\n",
    "        \n",
    "    return np.array(padded_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(train_inputs, val_inputs, train_labels, val_labels,\n",
    "                batch_size=50):\n",
    "    \"\"\"\n",
    "    Convert train and validation sets to torch.Tensors and load them to\n",
    "    DataLoader.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert data type to torch.Tensor\n",
    "    train_inputs, val_inputs, train_labels, val_labels =\\\n",
    "    tuple(torch.tensor(data) for data in\n",
    "          [train_inputs, val_inputs, train_labels, val_labels])\n",
    "\n",
    "    # Create DataLoader for training data\n",
    "    train_data = TensorDataset(train_inputs, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Create DataLoader for validation data\n",
    "    val_data = TensorDataset(val_inputs, val_labels)\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splice Dataset\n",
    "https://archive.ics.uci.edu/ml/datasets/Molecular+Biology+%28Promoter+Gene+Sequences%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset\n",
    "# all sequences put in seq\n",
    "# all labels in corresponding index in labels\n",
    "seqs = []\n",
    "labels = []\n",
    "\n",
    "f = open('splice.data')\n",
    "for line in f:\n",
    "    line = [i.replace(',', '') for i in line.split()]\n",
    "    seqs.append(line[2])\n",
    "    labels.append(line[0])\n",
    "f.close()\n",
    "\n",
    "# tokenizing and getting a vocab\n",
    "tokenized_seqs, voc2ind = prepare_data(seqs)\n",
    "\n",
    "# padding\n",
    "tokenized_seqs = pad(tokenized_seqs, voc2ind)\n",
    "\n",
    "# Showing the result of this:\n",
    "tokenized_seqs, voc2ind, set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n",
    "    tokenized_seqs, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Load data to PyTorch DataLoader\n",
    "train_dataloader, test_dataloader = data_loader(train_inputs, val_inputs, train_labels, val_labels, batch_size=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning2",
   "language": "python",
   "name": "deep_learning2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
